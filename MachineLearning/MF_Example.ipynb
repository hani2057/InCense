{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpXEqyCqTVllPUToy5dEsZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIoxk8DDE9aD","executionInfo":{"status":"ok","timestamp":1679903975749,"user_tz":-540,"elapsed":1019,"user":{"displayName":"[서울_16반_김도균]","userId":"07754711342267896363"}},"outputId":"e0f78bcd-dd71-4aa2-d40c-26408899c606"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 10 ; cost = 0.1588\n","Iteration: 20 ; cost = 0.1147\n","Iteration: 30 ; cost = 0.0964\n","Iteration: 40 ; cost = 0.0840\n","Iteration: 50 ; cost = 0.0742\n","Iteration: 60 ; cost = 0.0658\n","Iteration: 70 ; cost = 0.0585\n","Iteration: 80 ; cost = 0.0521\n","Iteration: 90 ; cost = 0.0465\n","Iteration: 100 ; cost = 0.0416\n","Iteration: 110 ; cost = 0.0372\n","Iteration: 120 ; cost = 0.0334\n","Iteration: 130 ; cost = 0.0300\n","Iteration: 140 ; cost = 0.0269\n","Iteration: 150 ; cost = 0.0242\n","Iteration: 160 ; cost = 0.0218\n","Iteration: 170 ; cost = 0.0197\n","Iteration: 180 ; cost = 0.0178\n","Iteration: 190 ; cost = 0.0161\n","Iteration: 200 ; cost = 0.0146\n","Iteration: 210 ; cost = 0.0133\n","Iteration: 220 ; cost = 0.0121\n","Iteration: 230 ; cost = 0.0110\n","Iteration: 240 ; cost = 0.0101\n","Iteration: 250 ; cost = 0.0092\n","Iteration: 260 ; cost = 0.0085\n","Iteration: 270 ; cost = 0.0078\n","Iteration: 280 ; cost = 0.0073\n","Iteration: 290 ; cost = 0.0067\n","Iteration: 300 ; cost = 0.0063\n","User Latent P:\n","[[-1.18372074 -0.7851521   0.40522592]\n"," [ 0.35827222 -0.7731675  -0.9021645 ]\n"," [-0.18403644  0.76814006  1.822946  ]\n"," [ 0.11459653  0.14110237  1.49818338]\n"," [ 1.07990528 -0.32408295  0.80400092]\n"," [ 0.56258625  0.37205889 -1.67112411]\n"," [-1.98180912 -0.46961209  0.89705377]]\n","Item Latent Q:\n","[[ 0.39926558 -2.04862945  1.50729929  0.83193865  0.30923481]\n"," [ 0.84158313 -1.26352656  0.53348158  1.79684449 -0.57354744]\n"," [-0.9479133   0.16694251 -0.08398663  0.43000994  1.26449544]]\n","P x Q:\n","[[-1.51750875  3.48471515 -2.23711918 -2.22132808  0.59668065]\n"," [ 0.34753477  0.09234104  0.2033226  -1.47914096 -0.58654441]\n"," [-1.15502046 -0.28921571 -0.02071252  2.01100611  1.80763168]\n"," [-1.25564413 -0.16294192  0.1221794   0.99311005  1.84895439]\n"," [-0.60369689 -1.66861642  1.38732285  0.66181668  1.53647674]\n"," [ 2.12182057 -1.9016187   1.1868245   0.41796922 -2.15255099]\n"," [-2.03681498  4.8031163  -3.31304941 -2.10682146  0.79082085]]\n","bias:\n","2.590909090909091\n","User Latent bias:\n","[ 0.19673957 -0.57273894 -0.10822988 -0.05195202  0.28897153  0.51973057\n","  0.06110203]\n","Item Latent bias:\n","[-0.28805475 -0.19967944  0.72662645  0.46447021 -0.39763944]\n","Final R matrix:\n","[[0.98208516 6.07268436 1.27715593 1.03079079 2.98668987]\n"," [2.07765018 1.91083176 2.94811921 1.00349941 1.03398631]\n"," [1.039604   1.99378407 3.18859315 4.95815554 3.89267145]\n"," [0.9952582  2.17633572 3.38776292 3.99653734 3.99027202]\n"," [1.98812899 1.01158476 4.99382992 4.00616751 4.01871792]\n"," [4.94440549 1.00934152 5.02409061 3.99307909 0.56044923]\n"," [0.32714139 7.25544798 0.06558816 1.00965988 3.04519253]]\n","Final RMSE:\n","0.0062774449175496344\n"]}],"source":["import numpy as np\n","\n","\n","class MatrixFactorization():\n","    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n","        \"\"\"\n","        :param R: rating matrix\n","        :param k: latent parameter\n","        :param learning_rate: alpha on weight update\n","        :param reg_param: beta on weight update\n","        :param epochs: training epochs\n","        :param verbose: print status\n","        \"\"\"\n","\n","        self._R = R\n","        self._num_users, self._num_items = R.shape\n","        self._k = k\n","        self._learning_rate = learning_rate\n","        self._reg_param = reg_param\n","        self._epochs = epochs\n","        self._verbose = verbose\n","\n","\n","    def fit(self):\n","        \"\"\"\n","        training Matrix Factorization : Update matrix latent weight and bias\n","\n","        참고: self._b에 대한 설명\n","        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n","        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n","\n","        :return: training_process\n","        \"\"\"\n","\n","        # init latent features\n","        self._P = np.random.normal(size=(self._num_users, self._k))\n","        self._Q = np.random.normal(size=(self._num_items, self._k))\n","\n","        # init biases\n","        self._b_P = np.zeros(self._num_users)\n","        self._b_Q = np.zeros(self._num_items)\n","        self._b = np.mean(self._R[np.where(self._R != 0)])\n","\n","        # train while epochs\n","        self._training_process = []\n","        for epoch in range(self._epochs):\n","\n","            # rating이 존재하는 index를 기준으로 training\n","            for i in range(self._num_users):\n","                for j in range(self._num_items):\n","                    if self._R[i, j] > 0:\n","                        self.gradient_descent(i, j, self._R[i, j])\n","            cost = self.cost()\n","            self._training_process.append((epoch, cost))\n","\n","            # print status\n","            if self._verbose == True and ((epoch + 1) % 10 == 0):\n","                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n","\n","\n","    def cost(self):\n","        \"\"\"\n","        compute root mean square error\n","        :return: rmse cost\n","        \"\"\"\n","\n","        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n","        # 참고: http://codepractice.tistory.com/90\n","        xi, yi = self._R.nonzero()\n","        predicted = self.get_complete_matrix()\n","        cost = 0\n","        for x, y in zip(xi, yi):\n","            cost += pow(self._R[x, y] - predicted[x, y], 2)\n","        return np.sqrt(cost) / len(xi)\n","\n","\n","    def gradient(self, error, i, j):\n","        \"\"\"\n","        gradient of latent feature for GD\n","\n","        :param error: rating - prediction error\n","        :param i: user index\n","        :param j: item index\n","        :return: gradient of latent feature tuple\n","        \"\"\"\n","\n","        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n","        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n","        return dp, dq\n","\n","\n","    def gradient_descent(self, i, j, rating):\n","        \"\"\"\n","        graident descent function\n","\n","        :param i: user index of matrix\n","        :param j: item index of matrix\n","        :param rating: rating of (i,j)\n","        \"\"\"\n","\n","        # get error\n","        prediction = self.get_prediction(i, j)\n","        error = rating - prediction\n","\n","        # update biases\n","        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n","        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n","\n","        # update latent feature\n","        dp, dq = self.gradient(error, i, j)\n","        self._P[i, :] += self._learning_rate * dp\n","        self._Q[j, :] += self._learning_rate * dq\n","\n","\n","    def get_prediction(self, i, j):\n","        \"\"\"\n","        get predicted rating: user_i, item_j\n","        :return: prediction of r_ij\n","        \"\"\"\n","        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n","\n","\n","    def get_complete_matrix(self):\n","        \"\"\"\n","        computer complete matrix PXQ + P.bias + Q.bias + global bias\n","\n","        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n","        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n","        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n","\n","        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n","\n","        :return: complete matrix R^\n","        \"\"\"\n","        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n","\n","\n","    def print_results(self):\n","        \"\"\"\n","        print fit results\n","        \"\"\"\n","\n","        print(\"User Latent P:\")\n","        print(self._P)\n","        print(\"Item Latent Q:\")\n","        print(self._Q.T)\n","        print(\"P x Q:\")\n","        print(self._P.dot(self._Q.T))\n","        print(\"bias:\")\n","        print(self._b)\n","        print(\"User Latent bias:\")\n","        print(self._b_P)\n","        print(\"Item Latent bias:\")\n","        print(self._b_Q)\n","        print(\"Final R matrix:\")\n","        print(self.get_complete_matrix())\n","        print(\"Final RMSE:\")\n","        print(self._training_process[self._epochs-1][1])\n","\n","\n","# run example\n","if __name__ == \"__main__\":\n","    # rating matrix - User X Item : (7 X 5)\n","    R = np.array([\n","        [1, 0, 0, 1, 3],\n","        [2, 0, 3, 1, 1],\n","        [1, 2, 0, 5, 0],\n","        [1, 0, 0, 4, 4],\n","        [2, 1, 5, 4, 0],\n","        [5, 1, 5, 4, 0],\n","        [0, 0, 0, 1, 0],\n","    ])\n","\n","    # P, Q is (7 X k), (k X 5) matrix\n","    factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=300, verbose=True)\n","    factorizer.fit()\n","    factorizer.print_results()"]},{"cell_type":"code","source":[],"metadata":{"id":"RLi3oHTwE_YH"},"execution_count":null,"outputs":[]}]}